2020-10-21 08:58:33 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 08:58:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Windows-10-10.0.17763-SP0
2020-10-21 08:58:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 7, 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'LOG_STDOUT': True, 'NEWSPIDER_MODULE': 'baidu.spiders', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 08:58:33 [scrapy.extensions.telnet] INFO: Telnet Password: 49919cf97298c477
2020-10-21 08:58:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 08:58:33 [stdout] INFO: ip中间件的执行
2020-10-21 08:58:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 08:58:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 08:58:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 08:58:33 [scrapy.core.engine] INFO: Spider opened
2020-10-21 08:58:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 08:58:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2020-10-21 08:58:33 [stdout] INFO: zhixingdaojieguo
2020-10-21 08:58:33 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 127, in _next_request
    request = next(slot.start_requests)
  File "D:\codefiler\baidu1.1\baidu\baidu\spiders\duanhaoma1.py", line 34, in start_requests
    phone = phone_dict[key].strip()
KeyError: 804
2020-10-21 08:58:33 [scrapy.core.engine] INFO: Closing spider (finished)
2020-10-21 08:58:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.0,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 10, 21, 0, 58, 33, 783062),
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'start_time': datetime.datetime(2020, 10, 21, 0, 58, 33, 783062)}
2020-10-21 08:58:33 [scrapy.core.engine] INFO: Spider closed (finished)
2020-10-21 09:00:48 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 09:00:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Windows-10-10.0.17763-SP0
2020-10-21 09:00:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 6, 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 09:00:48 [scrapy.extensions.telnet] INFO: Telnet Password: 008eb33be732dc84
2020-10-21 09:00:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 09:00:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 09:00:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 09:00:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 09:00:48 [scrapy.core.engine] INFO: Spider opened
2020-10-21 09:00:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:00:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2020-10-21 09:00:48 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 127, in _next_request
    request = next(slot.start_requests)
  File "D:\codefiler\baidu1.1\baidu\baidu\spiders\duanhaoma1.py", line 34, in start_requests
    phone = phone_dict[key].strip()
KeyError: 804
2020-10-21 09:00:48 [scrapy.core.engine] INFO: Closing spider (finished)
2020-10-21 09:00:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.008953,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 10, 21, 1, 0, 48, 836908),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2020, 10, 21, 1, 0, 48, 827955)}
2020-10-21 09:00:48 [scrapy.core.engine] INFO: Spider closed (finished)
2020-10-21 09:01:19 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 09:01:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Windows-10-10.0.17763-SP0
2020-10-21 09:01:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 8, 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 09:01:19 [scrapy.extensions.telnet] INFO: Telnet Password: 3e281fec7e4ce3d7
2020-10-21 09:01:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 09:01:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 09:01:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 09:01:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 09:01:19 [scrapy.core.engine] INFO: Spider opened
2020-10-21 09:01:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:01:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2020-10-21 09:01:19 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): webapi.http.zhimacangku.com:80
2020-10-21 09:01:19 [urllib3.connectionpool] DEBUG: http://webapi.http.zhimacangku.com:80 "GET /getip?num=1&type=1&pro=&city=0&yys=0&port=1&time=1&ts=0&ys=0&cs=0&lb=1&sb=0&pb=4&mr=1&regions= HTTP/1.1" 200 21
2020-10-21 09:01:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=13708495443> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=99908410&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:01:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=15367> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=80116455&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:01:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=13476> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=84013115&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:01:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1372192> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=31867226&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=13760802030> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=99347945&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=18023> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=16700515&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1317327> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=23221782&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:19 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:02:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=13888> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=58119020&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=15834> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=66902175&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1388400> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=1386071&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:02:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=189014> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=26883558&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:03:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=12315> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=46225123&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:03:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1300229> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=83774952&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:03:19 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:03:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1335380> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=9599105&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:03:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1361797> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=3932524&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:03:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1380238> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=84523761&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1774116> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=23429448&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=19821> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=79269965&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:11 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.baidu.com/s?&wd=1317327> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2020-10-21 09:04:19 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:04:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1340715> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=67901898&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1820525> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=26000260&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1702824> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=91923701&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1327787> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=10897744&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=14705> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=47499532&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:05:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1531828> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=59859893&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:05:19 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:05:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=157123> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=91985116&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:05:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1589208> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=63805000&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:05:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=18030> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=93780865&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:05:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=18202> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=26785170&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:06:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.baidu.com/s?&wd=1843859. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2020-10-21 09:06:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.baidu.com/s?&wd=1843859> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'BODY' state, still expecting more data to get to 'FINISHED' state.>]
2020-10-21 09:06:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1843859> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=95435868&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:06:19 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 09:06:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1389210> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=19443731&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 09:06:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?&wd=1916826> (referer: https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=55691936&fenlei=256&rsv_pq=d1800bac00058ab0&rsv_t=0dd3C%2B%2BhcSELg7hy9azb5dKxdWJruGoIIcDBr6BW1%2FuKulgDrC5Gebzbpck&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=6&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=i&inputT=4274&rsv_sug4=4274)
2020-10-21 11:01:07 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 11:01:07 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0
2020-10-21 11:01:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 9, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 11:01:07 [scrapy.extensions.telnet] INFO: Telnet Password: dbbee625312a4685
2020-10-21 11:01:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 11:01:07 [duanhaoma] INFO: Reading start URLs from redis key 'dhm:start_urls' (batch size: 1, encoding: utf-8
2020-10-21 11:01:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 11:01:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 11:01:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 11:01:07 [scrapy.core.engine] INFO: Spider opened
2020-10-21 11:01:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:01:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-10-21 11:01:07 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C86B730> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:12 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7950> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:17 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A79D8> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:22 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7A60> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:27 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7AE8> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:32 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7B70> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:37 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7BF8> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:42 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7C80> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:47 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7D08> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:52 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7D90> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:01:57 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7E18> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:02 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7EA0> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:02:07 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A7F28> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:12 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8A78C8> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:17 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D50D0> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:22 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5158> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:27 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D51E0> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:32 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5268> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:37 [scrapy_redis.dupefilter] DEBUG: Filtered duplicate request <GET https://www.baidu.com/s?&wd=1317327> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2020-10-21 11:02:37 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D52F0> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:42 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5378> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:47 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5400> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:52 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5488> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:57 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\crawler.py", line 309, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1272, in run
    self.mainLoop()
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 1281, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "c:\programdata\anaconda3\lib\site-packages\twisted\internet\base.py", line 902, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 135, in _next_request
    self.crawl(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\scheduler.py", line 167, in enqueue_request
    self.queue.push(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 99, in push
    data = self._encode_request(request)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy_redis\queue.py", line 42, in _encode_request
    obj = request_to_dict(request, self.spider)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 19, in request_to_dict
    cb = _find_method(spider, cb)
  File "c:\programdata\anaconda3\lib\site-packages\scrapy\utils\reqser.py", line 99, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
builtins.ValueError: Function <function DuanhaomaSpider.start_requests.<locals>.<lambda> at 0x0000027E9C8D5510> is not a method of: <DuanhaomaSpider 'duanhaoma' at 0x27e9c27e940>

2020-10-21 11:02:59 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-10-21 11:02:59 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-10-21 11:02:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 112.003752,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 10, 21, 3, 2, 59, 934638),
 'log_count/CRITICAL': 23,
 'log_count/DEBUG': 1,
 'log_count/INFO': 13,
 'scheduler/enqueued/redis': 23,
 'start_time': datetime.datetime(2020, 10, 21, 3, 1, 7, 930886)}
2020-10-21 11:02:59 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-10-21 11:03:00 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 11:03:00 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0
2020-10-21 11:03:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 11:03:00 [scrapy.extensions.telnet] INFO: Telnet Password: 98efeafc855d1708
2020-10-21 11:03:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 11:03:00 [duanhaoma] INFO: Reading start URLs from redis key 'dhm:start_urls' (batch size: 1, encoding: utf-8
2020-10-21 11:03:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 11:03:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 11:03:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 11:03:01 [scrapy.core.engine] INFO: Spider opened
2020-10-21 11:03:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:03:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-10-21 11:03:27 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-10-21 11:03:27 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-10-21 11:03:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 26.504366,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 10, 21, 3, 3, 27, 650343),
 'log_count/INFO': 12,
 'start_time': datetime.datetime(2020, 10, 21, 3, 3, 1, 145977)}
2020-10-21 11:03:27 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-10-21 11:03:34 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 11:03:34 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0
2020-10-21 11:03:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 11:03:34 [scrapy.extensions.telnet] INFO: Telnet Password: c4cabb9a01c2b215
2020-10-21 11:03:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 11:03:34 [duanhaoma] INFO: Reading start URLs from redis key 'dhm:start_urls' (batch size: 1, encoding: utf-8
2020-10-21 11:03:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 11:03:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 11:03:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 11:03:34 [scrapy.core.engine] INFO: Spider opened
2020-10-21 11:03:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:03:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-10-21 11:04:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:05:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:06:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:07:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:08:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:09:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:10:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:14:05 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: baidu)
2020-10-21 11:14:05 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0
2020-10-21 11:14:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'baidu', 'CONCURRENT_REQUESTS': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 7, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'log/scrapy_2020_10_21.log', 'NEWSPIDER_MODULE': 'baidu.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['baidu.spiders']}
2020-10-21 11:14:05 [scrapy.extensions.telnet] INFO: Telnet Password: 5d3ebe454962ab31
2020-10-21 11:14:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-10-21 11:14:05 [duanhaoma] INFO: Reading start URLs from redis key 'dhm:start_urls' (batch size: 1, encoding: utf-8
2020-10-21 11:14:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'baidu.middlewares.RandomRefererMiddleware',
 'baidu.middlewares.ChangeIpMiddleware',
 'baidu.middlewares.HeaderscanshuMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-10-21 11:14:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-10-21 11:14:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-10-21 11:14:05 [scrapy.core.engine] INFO: Spider opened
2020-10-21 11:14:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-10-21 11:14:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-10-21 11:14:24 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-10-21 11:14:24 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-10-21 11:14:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 19.009667,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 10, 21, 3, 14, 24, 537602),
 'log_count/INFO': 12,
 'start_time': datetime.datetime(2020, 10, 21, 3, 14, 5, 527935)}
2020-10-21 11:14:24 [scrapy.core.engine] INFO: Spider closed (shutdown)
